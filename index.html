<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="keywords" content="Jincheng Mei">
<meta name="description" content="Jincheng Mei's home page">

<title>Jincheng Mei</title>

<link rel="stylesheet" type="text/css" href="files/p4p-2.0.css">

</head>

<body>

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
        <td width="220">
            <img src="files/photo.jpeg" border="0", width="130">
        </td>
        <td>
            <div id="toptitle">					
                <h1>Jincheng Mei, &nbsp; <small>Ph.D.</small></h1></div>
                <br><br>

                Research Scientist<br>
                Google DeepMind, Canada
                <p>
                <br><br>

                jcmei514 AT gmail.com (personal email)<br>
                jcmei AT google.com (for Google related work)<br><br>
                
                </p>
        </td>
        </tr>
        <tr>
        </tr>
    </tbody>
</table>

<h2>Bio</h2>
<p>
I am a research scientist at <a href="https://www.deepmind.com/">Google DeepMind</a> Canada (previously at <a href="https://research.google/teams/brain/">Google Brain</a> until April 20, 2023), doing research in the area of machine learning, reinforcement learning, and optimization.
I obtained my Ph.D. degree (09/2015-09/2021) from the University of Alberta, 
under the supervision of <a href="http://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a>. 
I have interned at Google Brain (08/2019-08/2021) and Borealis AI (09/2018-01/2019 and 03/2019-05/2019).
</p>

<h2>Education</h2>

<ul>

    <li>
        Sep. 2021: Ph.D. (in Statistical Machine Learning program), Department of Computing Science, University of Alberta.<br>
        Supervisor: Dale Schuurmans.
    </li>

    <li>
        Mar. 2015: M.S., Department of Computer Science and Engineering, Shanghai Jiao Tong University.<br>
        Supervisor: Bao-Liang Lu.
    </li>

    <li>
        June 2012: B.E., School of Software Engineering, South China University of Technology.
    </li>


</ul>


<h2>Research</h2>

* indicates equal contribution.<br>
† indicates equal advising.

<br/>

<ul>

    <li>
        Ordering-based Conditions for Global Convergence of Policy Gradient Methods.<br>
        <b>Jincheng Mei</b>, Bo Dai, Alekh Agarwal, Mohammad Ghavamzadeh, Csaba Szepesvári, and Dale Schuurmans.<br>
        <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2023.<br>
        <font color="red">Oral Presentation</font><br>
        <p style="margin-top:3px">
        </p>
    </li>

    <li>
        Stochastic Gradient Succeeds for Bandits.<br>
        <b>Jincheng Mei</b>*, Zixin Zhong*, Bo Dai, Alekh Agarwal, Csaba Szepesvári, and Dale Schuurmans.<br>
        <em>International Conference on Machine Learning</em> (<b>ICML</b>), 2023.<br>
        <p style="margin-top:3px">
        [<a href="https://proceedings.mlr.press/v202/mei23a.html">Paper</a>]
        </p>
    </li>

    <li>
        Regularization and Variance-Weighted Regression Achieves Minimax Optimality in Linear MDPs: Theory and Practice.<br>
        Toshinori Kitamura, Tadashi Kozuno, Yunhao Tang, Nino Vieillard, Michal Valko, Wenhao Yang, <b>Jincheng Mei</b>, Pierre Ménard, Mohammad Gheshlaghi Azar, Rémi Munos, Olivier Pietquin, Matthieu Geist, Csaba Szepesvári, Wataru Kumagai, and Yutaka Matsuo.<br>
        <em>International Conference on Machine Learning</em> (<b>ICML</b>), 2023.<br>
        <p style="margin-top:3px">
        [<a href="https://proceedings.mlr.press/v202/kitamura23a.html">Paper</a>][<a href="https://arxiv.org/abs/2305.13185">arXiv</a>]
        </p>
    </li>


    <li>
        The Role of Baselines in Policy Gradient Optimization.<br>
        <b>Jincheng Mei</b>, Wesley Chung, Valentin Thomas, Bo Dai, Csaba Szepesvári, and Dale Schuurmans.<br>
        <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2022.<br>
        <p style="margin-top:3px">
        [<a href="https://arxiv.org/abs/2301.06276">arXiv</a>][<a href="https://openreview.net/forum?id=XzeTJBq1Ce2">OpenReview</a>]
        </p>
    </li>

    <li>
        On the Global Convergence Rates of Decentralized Softmax Gradient Play in Markov Potential Games.<br>
        Runyu Zhang, <b>Jincheng Mei</b>, Bo Dai, Dale Schuurmans, and Na Li.<br>
        <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2022.<br>
        <p style="margin-top:3px">
        [<a href="https://arxiv.org/abs/2202.00872">arXiv</a>][<a href="https://openreview.net/forum?id=X1oVDZIABwF">OpenReview</a>]
        </p>
    </li>


    <li>
        KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal.<br>
        Tadashi Kozuno, Wenhao Yang, Nino Vieillard, Toshinori Kitamura, Yunhao Tang, <b>Jincheng Mei</b>, Pierre Ménard, Mohammad Gheshlaghi Azar, Michal Valko, Rémi Munos, Olivier Pietquin, Matthieu Geist, and Csaba Szepesvári.<br>
        <em>Preprint</em>, 2022.<br>
        <p style="margin-top:3px">
        [<a href="https://arxiv.org/abs/2205.14211">arXiv</a>]
        </p>
    </li>


    <li>
        Understanding and Leveraging Overparameterization in Recursive Value Estimation.<br>
        Chenjun Xiao, Bo Dai, <b>Jincheng Mei</b>, Oscar Ramirez, Ramki Gummadi, Chris Harris, and Dale Schuurmans.<br>
        <em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2022.<br>
        <p style="margin-top:3px">
        [<a href="https://openreview.net/forum?id=shbAgEsk3qM">OpenReview</a>]
        </p>
    </li>

    <li>
        Understanding and Mitigating the Limitations of Prioritized Experience Replay.<br>
        Yangchen Pan*, <b>Jincheng Mei</b>*, Amir-massoud Farahmand, Martha White, Hengshuai Yao, Mohsen Rohani, and Jun Luo.<br>
        <em>Conference on Uncertainty in Artificial Intelligence</em> (<b>UAI</b>), 2022.<br>
        <p style="margin-top:3px">
        [<a href=https://arxiv.org/abs/2007.09569>arXiv</a>][<a href="https://openreview.net/forum?id=HBlNGvIicg9">OpenReview</a>]
        </p>
    </li>

    <li>
        Non-uniform Analysis for Non-convex Optimization in Machine Learning.<br>
        <b>Jincheng Mei</b>.<br>
        <em>Ph.D. Thesis</em>, Department of Computing Science, University of Alberta, 2021.<br>
        <font color="red">Best Ph.D. Dissertation Award</font>, Canadian Artificial Intelligence Association, 2022.<br>
        <p style="margin-top:3px">
        [<a href="https://era.library.ualberta.ca/items/e0422208-4d5a-4c52-912a-4a34e5a06de1">Thesis</a>]
        </p>
    </li>

    <li>
        Understanding the Effect of Stochasticity in Policy Optimization.<br> 
        <b>Jincheng Mei</b>, Bo Dai, Chenjun Xiao, Csaba Szepesvári†, and Dale Schuurmans†.<br>
        <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2021.<br>
        <p style="margin-top:3px">
        [<a href="https://arxiv.org/abs/2110.15572">arXiv</a>][<a href="https://openreview.net/forum?id=jdugaf8i4ne">OpenReview</a>]
        </p>
    </li>

    <li>
        Leveraging Non-uniformity in First-order Non-convex Optimization.<br>
        <b>Jincheng Mei</b>*, Yue Gao*, Bo Dai, Csaba Szepesvári, and Dale Schuurmans.<br>
        <em>International Conference on Machine Learning</em> (<b>ICML</b>), 2021.<br>
        <p style="margin-top:3px">
        [<a href=http://proceedings.mlr.press/v139/mei21a.html>Paper</a>][<a href=https://arxiv.org/abs/2105.06072>arXiv</a>]
        </p>
    </li>

    <li>
        On the Optimality of Batch Policy Optimization Algorithms.<br>
        Chenjun Xiao*, Yifan Wu*, Tor Lattimore, Bo Dai, <b>Jincheng Mei</b>, Lihong Li, Csaba Szepesvári, and Dale Schuurmans.<br>
        <em>International Conference on Machine Learning</em> (<b>ICML</b>), 2021.<br>
        <p style="margin-top:3px">
        [<a href=http://proceedings.mlr.press/v139/xiao21b.html>Paper</a>][<a href=https://arxiv.org/abs/2104.02293>arXiv</a>]
        </p>
    </li>

    <li>
        Escaping the Gravitational Pull of Softmax.<br>
        <b>Jincheng Mei</b>, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesvári, and Dale Schuurmans.<br>
        <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2020.<br>
        <font color="red">Oral Presentation</font><br>
        <p style="margin-top:3px">
        [<a href=https://proceedings.neurips.cc/paper/2020/hash/f1cf2a082126bf02de0b307778ce73a7-Abstract.html>Paper</a>]
        </p> 
    </li>

    <li>
        On the Global Convergence Rates of Softmax Policy Gradient Methods.<br>
        <b>Jincheng Mei</b>, Chenjun Xiao, Csaba Szepesvári, and Dale Schuurmans.<br>
        <em>International Conference on Machine Learning</em> (<b>ICML</b>), 2020.<br>
        <p style="margin-top:3px">
        [<a href=http://proceedings.mlr.press/v119/mei20b.html>Paper</a>][<a href=http://arxiv.org/abs/2005.06392>arXiv</a>]
        </p> 
    </li>


    <li>
        Frequency-based Search-control in Dyna.<br>
        Yangchen Pan*, <b>Jincheng Mei</b>*, and Amir-massoud Farahmand.<br>
        <em>International Conference on Learning Representations</em> (<b>ICLR</b>), 2020.<br>
        <p style="margin-top:3px">
        [<a href=https://openreview.net/forum?id=B1gskyStwr>Paper</a>]
        </p>
    </li>

    <li>
        Maximum Entropy Monte-Carlo Planning.<br>
        Chenjun Xiao, <b>Jincheng Mei</b>, Ruitong Huang, Dale Schuurmans, and Martin Müller.<br>
        <em>Advances in Neural Information Processing Systems</em> (<b>NeurIPS</b>), 2019.<br>
        <p style="margin-top:3px">
        [<a href=http://papers.nips.cc/paper/9148-maximum-entropy-monte-carlo-planning>Paper</a>]
        </p>
    </li>

    <li>
        On Principled Entropy Exploration in Policy Optimization.<br>
        <b>Jincheng Mei</b>*, Chenjun Xiao*, Ruitong Huang, Dale Schuurmans, and Martin Müller.<br>
        <em>International Joint Conference on Artificial Intelligence</em> (<b>IJCAI</b>), 2019.<br>
        <p style="margin-top:3px">
        [<a href=https://www.ijcai.org/proceedings/2019/434>Paper</a>][<a href=files/ijcai19_principled_entropy_exploration_long_version.pdf>Long version</a>]
        </p>
    </li>

    <li>
        Memory-Augmented Monte Carlo Tree Search.<br>
        Chenjun Xiao, <b>Jincheng Mei</b>, and Martin Müller.<br>
        <em>AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>), 2018.<br>
        <font color="red">Outstanding Paper Award</font><br>
        <p style="margin-top:3px">
        [<a href=https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17139>Paper</a>]
        </p>
    </li>

    <li>
        Identifying and Tracking Sentiments and Topics from Social Media Texts during Natural Disasters.<br>
        Min Yang, <b>Jincheng Mei</b>, Heng Ji, Wei Zhao, Zhou Zhao, and Xiaojun Chen.<br>
        <em>International Conference on Empirical Methods in Natural Language Processinge</em> (<b>EMNLP</b>), 2017.<br>
        <p style="margin-top:3px">
        [<a href="http://www.aclweb.org/anthology/D17-1055">Paper</a>]
        </p>
    </li>

    <li>
        Discovering Author Interest Evolution in Topic Modeling.<br>
        Min Yang, <b>Jincheng Mei</b>, Fei Xu, Wenting Tu, and Ziyu Lu.<br>
        <em>International ACM SIGIR conference on Research and Development in Information Retrieval</em> (<b>SIGIR</b>), 2016.<br>
        <p style="margin-top:3px">
        [<a href="https://dl.acm.org/citation.cfm?id=2914723">Paper</a>]
        </p>
    </li>
    
    <li>
        On the Reducibility of Submodular Functions.<br>
        <b>Jincheng Mei</b>, Hao Zhang, and Bao-Liang Lu.<br>
        <em>International Conference on Artificial Intelligence and Statistics</em> (<b>AISTATS</b>), 2016.<br>
        <p style="margin-top:3px">
        [<a href="http://jmlr.org/proceedings/papers/v51/mei16.html">Paper</a>]
        </p>
    </li>

    <li>
        On Unconstrained Quasi-Submodular Function Optimization.<br>
        <b>Jincheng Mei</b>, Kang Zhao, and Bao-Liang Lu.<br>
        <em>AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>), 2015.<br>
        <p style="margin-top:3px">
        [<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9369">Paper</a>]
        </p>
    </li>
	
    <li>
        Locality Preserving Hashing.<br>
        Kang Zhao, Hongtao Lu, and <b>Jincheng Mei</b>.<br>
        <em>AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>), 2014.<br>
        <p style="margin-top:3px">
        [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/9133">Paper</a>]
        </p>
    </li>
	
    <li>
        Saliency Level Set Evolution.<br>
        <b>Jincheng Mei</b> and Bao-Liang Lu.<br>
        <em>International Conference on Neural Information Processing</em> (<b>ICONIP</b>), 2014.<br>
        <p style="margin-top:3px">
        [<a href="http://link.springer.com/chapter/10.1007%2F978-3-319-12640-1_21">Paper</a>]
        </p>
    </li>

</ul>


</div>

</body>
</html>

